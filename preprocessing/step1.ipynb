{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 data preprocessing\n",
    "\n",
    "part (a): tabular data\n",
    "\n",
    "the zero-shot ability of our model comes from embedding of feature description\n",
    "\n",
    "First we should extract the name of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export http_proxy=http://localhost:1080\n",
    "!export https_proxy=http://localhost:1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "ic.enable()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60456/1411125147.py:20: DtypeWarning: Columns (126) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tabular_data = pd.read_csv(tabular_data_path)\n"
     ]
    }
   ],
   "source": [
    "### NACC dataset\n",
    "\n",
    "tabular_data_path = \"/openbayes/home/LMTDE/data/datasets/nacc_new/naccImg_validation_normed.csv\" # can be any tabular data file, e.g. CSV, TSV, etc.\n",
    "pkl_path = \"/openbayes/home/LMTDE/data/datasets/nacc_new/meta/rewrite3/column_info.pkl\"\n",
    "background = \"Funded by NIA, the National Alzheimer's Coordinating Center (NACC) has developed and maintains a large relational database of standardized clinical and neuropathological research data. NACC provides a valuable resource for both exploratory and explanatory Alzheimer's disease research. NACC data are freely available to all researchers.\"\n",
    "\n",
    "###\n",
    "\n",
    "### ADNI dataset\n",
    "\n",
    "# tabular_data_path = \"/openbayes/home/LMTDE/data/datasets/adni/adni_final.csv\"\n",
    "# pkl_path = \"/openbayes/home/LMTDE/data/datasets/adni/meta/rewrite3/column_info.pkl\"\n",
    "# background = \"The ADNI data set is a comprehensive and widely used collection of longitudinal clinical, imaging, genetic, and other biomarker data. It encompasses various data types, including structural, functional, and molecular brain imaging, biofluid biomarkers, cognitive assessments, genetic data, and demographic information.ADNI participants are assigned to a Schedule of Events (SOE) based on various factors, such as clinical diagnosis (unimpaired, MCI, AD/dementia). During each visit, the SOE dictates what data is collected. This data is made available to approved researchers.\"\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "# read the data into a pandas DataFrame\n",
    "tabular_data = pd.read_csv(tabular_data_path)\n",
    "\n",
    "\n",
    "\n",
    "def save_pkl(column_info, pkl_path=pkl_path, step=0):\n",
    "    # add step to pkl path if step > 0\n",
    "    if step > 0:\n",
    "        pkl_path = pkl_path.replace(\".pkl\", f\"_step{step}.pkl\")\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(column_info, f)\n",
    "    print(f\"column info saved to {pkl_path}\")\n",
    "\n",
    "def load_pkl(pkl_path=pkl_path, step=0):\n",
    "    if step > 0:\n",
    "        pkl_path = pkl_path.replace(\".pkl\", f\"_step{step}.pkl\")\n",
    "    if not os.path.exists(pkl_path):\n",
    "        print(f\"column info not found at {pkl_path}\")\n",
    "        return None\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        column_info = pickle.load(f)\n",
    "    return column_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, classify the data to {\"categorical\", \"numerical\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column info saved to /openbayes/home/LMTDE/data/datasets/nacc_new/meta/rewrite3/column_info_step1.pkl\n",
      "column info saved to /openbayes/home/LMTDE/data/datasets/nacc_new/meta/rewrite3/column_info_step1.pkl\n"
     ]
    }
   ],
   "source": [
    "def what_type_of_data(tabular_data, column_name):\n",
    "    # \"img_emb\": column name contains \"img_\"\n",
    "    # \"numerical\": data type in [int, float] and (unique values > 32 or any float data changes after rounding to 0 decimal places)\n",
    "    # \"categorical\": else\n",
    "    if \"img_\" in column_name:\n",
    "        return \"img_emb\"\n",
    "    if column_name in [\"bat_LOGIMO\", \"bat_LOGIDAY\", \"bat_OTRLALI\"]:\n",
    "        return \"numerical\"\n",
    "    if tabular_data[column_name].dtype in [int, float]:\n",
    "        if tabular_data[column_name].nunique() > 10: # 32:\n",
    "            return \"numerical\"\n",
    "        # if any float data changes after rounding to 0 decimal places, then it is numerical\n",
    "        if not tabular_data[column_name].apply(lambda x: round(x, 0)).equals(tabular_data[column_name]):\n",
    "            return \"numerical\"\n",
    "    return \"categorical\"\n",
    "\n",
    "# store the data type of each column in a dictionary\n",
    "column_info = {\n",
    "    column_name: {\n",
    "        \"type\": what_type_of_data(tabular_data, column_name),\n",
    "        \"num_categories\": tabular_data[column_name].nunique()\n",
    "    } for column_name in tabular_data.columns\n",
    "}\n",
    "\n",
    "save_pkl(column_info, step=1)\n",
    "\n",
    "\n",
    "# if dont have standard toml, ignore following check\n",
    "# check using toml file if have\n",
    "nacc_toml_path = \"/openbayes/home/LMTDE/data/datasets/nacc_new/meta/conf_mri.toml\"\n",
    "import toml\n",
    "with open(nacc_toml_path, \"r\") as f:\n",
    "    column_type = toml.load(f)\n",
    "    # check if column type match\n",
    "    for column_name in column_type['feature']:\n",
    "        if column_name not in column_info:\n",
    "            # print(f\"column {column_name} not found in the data\")\n",
    "            continue\n",
    "        if column_info[column_name]['type'] != column_type['feature'][column_name]['type']:\n",
    "            # if category - binary and num_categories=2, match\n",
    "            # if category - multi and num_categories>2, match\n",
    "            if column_info[column_name]['type'] == \"categorical\" and column_type['feature'][column_name]['type'] == \"binary\" and column_info[column_name]['num_categories'] == 2:\n",
    "                continue\n",
    "            if column_info[column_name]['type'] == \"categorical\" and column_type['feature'][column_name]['type'] == \"multiple\" and column_info[column_name]['num_categories'] > 2:\n",
    "                continue\n",
    "            print(f\"column {column_name} type not match, expect {column_type['feature'][column_name]['type']} but got {column_info[column_name]['type']}\")\n",
    "\n",
    "    # drop columns not in column_type\n",
    "    column_info = {column_name: column_info[column_name] for column_name in column_info.keys() if column_name in column_type['feature']}\n",
    "\n",
    "save_pkl(column_info, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we rewrite and concatenate the description and category (for categorical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/363 [00:00<?, ?it/s]/tmp/ipykernel_7726/4199719960.py:18: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
      "/tmp/ipykernel_7726/4199719960.py:20: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run(background=background, description=description, rewrite_count=rewrite_count)\n",
      "100%|██████████| 363/363 [28:07<00:00,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column info saved to /openbayes/home/LMTDE/data/datasets/nacc_new/meta/rewrite3/column_info_step2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def rewrite_column_description(background, description, rewrite_count=1):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"background\", \"description\", \"rewrite_count\"],\n",
    "        template=\"\"\"\n",
    "        [Background]\n",
    "        Background information: {background}\n",
    "\n",
    "        [Instruction]\n",
    "        Please comprehend and then rewrite the following raw text (which may be a JSON-style string including detailed explanation, or contain only uppercase letters representing abbreviations, in which case you need to infer their meanings based on background knowledge) into a declarative sentence that is easy for humans to understand. Ensure to include the meaning of every possible data value (if provided in the raw text). Rewrite it in {rewrite_count} different ways or styles. (Separate the results with a newline character, no empty lines, and no serial numbers before the results.) Do not include the original column name (the string of uppercase letters) in your rewritten results; express the meaning using normal language instead.\n",
    "\n",
    "        [Raw text]\n",
    "        {description}\n",
    "        \"\"\"\n",
    "    )\n",
    "    # ic(background, description, prompt_template)\n",
    "\n",
    "    llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    while True:\n",
    "        result = chain.run(background=background, description=description, rewrite_count=rewrite_count)\n",
    "        rewritten_descriptions = result.strip().split(\"\\n\")\n",
    "        # drop empty line\n",
    "        rewritten_descriptions = [desc for desc in rewritten_descriptions if desc]\n",
    "        if len(rewritten_descriptions) >= rewrite_count:\n",
    "            break\n",
    "    return rewritten_descriptions[:rewrite_count]\n",
    "\n",
    "if 'nacc' in tabular_data_path:\n",
    "    # use additional information to rewrite the column descriptions\n",
    "    addfile_path = \"/openbayes/home/LMTDE/data/datasets/nacc_new/uds3_rdd_origin.xlsx\"\n",
    "    # for each row in addfile, 1th, 2th, ..., 6th column means: VariableName, Form, VariableType, ShortDescriptor, DataType, AllowableCodes\n",
    "    # create a dictionary to store, key=VariableName, value={\"VariableName\":..., \"Form\":..., ...}(json data)\n",
    "    addfile_data = pd.read_excel(addfile_path, sheet_name=\"uds3-rdd\", header=None)\n",
    "    addfile_column_name = addfile_data.iloc[:, 0].values\n",
    "    addfile_info = {}\n",
    "    for i in range(addfile_data.shape[0]):\n",
    "        column_name = addfile_data.iloc[i, 0]\n",
    "        addfile_info[column_name] = {}\n",
    "        addfile_info[column_name][\"origin_column_name\"] = addfile_data.iloc[i, 0]\n",
    "        addfile_info[column_name][\"form\"] = addfile_data.iloc[i, 1]\n",
    "        addfile_info[column_name][\"variable_type\"] = addfile_data.iloc[i, 2]\n",
    "        addfile_info[column_name][\"description\"] = addfile_data.iloc[i, 3]\n",
    "        addfile_info[column_name][\"data_type\"] = addfile_data.iloc[i, 4]\n",
    "        addfile_info[column_name][\"allowable_codes\"] = addfile_data.iloc[i, 5]\n",
    "\n",
    "elif 'adni' in tabular_data_path:\n",
    "    addfile_path = \"/openbayes/home/LMTDE/data/datasets/adni/meta/adni_all_descriptor.csv\"\n",
    "    addfile_data = pd.read_csv(addfile_path)\n",
    "    # get the field name from first row\n",
    "    field_names = addfile_data.columns.tolist()\n",
    "    # ic(field_names)\n",
    "    # for each row\n",
    "    addfile_column_name = addfile_data.iloc[:, 3].values\n",
    "    # ic(addfile_column_name)\n",
    "    addfile_info = {}\n",
    "    for i in range(addfile_data.shape[0]):\n",
    "        column_name = addfile_data.iloc[i, 3]\n",
    "        if column_name not in addfile_info:\n",
    "            addfile_info[column_name] = {}\n",
    "        for j in range(len(field_names)):\n",
    "            # if old info is nan or empty, update it\n",
    "            if field_names[j] not in addfile_info[column_name] or pd.isna(addfile_info[column_name][field_names[j]]) or addfile_info[column_name][field_names[j]] == \"\":\n",
    "                addfile_info[column_name][field_names[j]] = addfile_data.iloc[i, j]\n",
    "\n",
    "else:\n",
    "    assert False, \"tabular_data_path should contain 'nacc' or 'adni'\"\n",
    "\n",
    "    \n",
    "def find_the_most_related_origin_column(column_name, addfile_column_name):\n",
    "    if column_name in addfile_column_name:\n",
    "        # return json string(dump from dict: addfile_info[column_name])\n",
    "        return json.dumps(addfile_info[column_name])\n",
    "    # if column_name is not in addfile_column_name, find the most related column\n",
    "    # use maximum common substring to find the most related column\n",
    "    max_common_substring = \"\"\n",
    "    most_related_column = \"\"\n",
    "    for origin_column_name in addfile_column_name:\n",
    "        if len(origin_column_name) < 5:\n",
    "            continue\n",
    "        for i in range(len(column_name) - 4):\n",
    "            for j in range(i + 5, len(column_name) + 1):\n",
    "                if column_name[i:j] in origin_column_name:\n",
    "                    if j - i > len(max_common_substring):\n",
    "                        max_common_substring = column_name[i:j]\n",
    "                        most_related_column = origin_column_name\n",
    "    # ic(column_name, most_related_column, max_common_substring)\n",
    "    if most_related_column:\n",
    "        return json.dumps(addfile_info[most_related_column])\n",
    "    return column_name\n",
    "\n",
    "\n",
    "column_info = load_pkl(step=1)\n",
    "\n",
    "\n",
    "for column_name in tqdm(column_info):\n",
    "    # description = column_name # you can use other detailed description \n",
    "    description = find_the_most_related_origin_column(column_name, addfile_column_name)\n",
    "\n",
    "    if 'adni' in tabular_data_path:\n",
    "        # based on addfile, use \"CODE\" field to determine the data type\n",
    "        if column_name in addfile_info:\n",
    "            if \"CODE\" in addfile_info[column_name]:\n",
    "                code_str = addfile_info[column_name][\"CODE\"]\n",
    "                # code_str could be nan or empty, skip it\n",
    "                if not pd.isna(code_str) and code_str != \"\":\n",
    "                    if \"..\" in code_str:\n",
    "                        column_info[column_name][\"type\"] = \"numerical\"\n",
    "                    elif \"Step=\" in code_str:\n",
    "                        column_info[column_name][\"type\"] = \"numerical\"\n",
    "    \n",
    "    rewritten_descriptions = rewrite_column_description(background, description, rewrite_count=3)\n",
    "    column_info[column_name][\"rewritten_descriptions\"] = rewritten_descriptions\n",
    "\n",
    "\n",
    "save_pkl(column_info, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_and_rewrite(column_description, category_description, expected_lines=None):\n",
    "    if len(category_description) > 2000:\n",
    "        ic(\"category_description is too long, rejected\")\n",
    "        ic(column_description)\n",
    "        return [column_description]\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"column_description\", \"category_description\", \"expected_lines\"],\n",
    "        template=\"\"\"\n",
    "        [Instruction]\n",
    "        The following text describes one of the columns in the tabular data (categorical data) and includes the category descriptions that actually exist in the real data (some data mentioned in the column description may not exist in the real data). Please concatenate the column description with each of the category descriptions and rewrite it as a declarative sentence (utilizing the description of the category it belongs to). \n",
    "        1. Because the real data contains {expected_lines} categories, you should output exactly {expected_lines} lines. Do not output these results in only one line. (Separate the results with a line break, no empty lines, and no serial numbers before the results.) \n",
    "        2. Do not include the original column name (the string of uppercase letters) in your rewritten results; express the meaning using normal language instead.\n",
    "\n",
    "        [Column description]\n",
    "        {column_description}\n",
    "\n",
    "        [Category description]\n",
    "        {category_description}\n",
    "        \"\"\"\n",
    "    )\n",
    "    #  Limit each response to 30 words or fewer. If there are particularly many categories (more than 10), limit each response to 15 words or fewer.\n",
    "    #  Do not include the original column name (the string of uppercase letters) in your rewritten results; express the meaning using normal language instead.\n",
    "    #  Later this rewrite result will be used to concatenate with each category data, so you should preserve enough important information.\n",
    "    llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    result = chain.run(column_description=column_description, category_description=category_description, expected_lines=expected_lines)\n",
    "    rewritten_descriptions = result.strip().split(\"\\n\")\n",
    "    # drop empty line\n",
    "    rewritten_descriptions = [desc for desc in rewritten_descriptions if desc]\n",
    "    return rewritten_descriptions\n",
    "\n",
    "\n",
    "column_info = load_pkl(step=2)\n",
    "give_up_columns = []\n",
    "for column_name in tqdm(column_info):\n",
    "    if column_info[column_name][\"type\"] == \"categorical\":\n",
    "        all_possible_data = tabular_data[column_name].dropna().unique()\n",
    "        all_possible_data.sort()\n",
    "        all_possible_data = [str(data) for data in all_possible_data]\n",
    "        category_description = \"The column contains the following categories: \" + \", \".join(all_possible_data)\n",
    "        assert(len(all_possible_data) == column_info[column_name][\"num_categories\"])\n",
    "\n",
    "        all_concate_result_for_different_rewrite = []\n",
    "        for description in column_info[column_name][\"rewritten_descriptions\"]:\n",
    "            if len(all_possible_data) > 10:\n",
    "                rewritten_descriptions = [description]\n",
    "                retry_count = 101\n",
    "                give_up_columns.append(column_name)\n",
    "            else:\n",
    "                retry_count = 0\n",
    "                while True:\n",
    "                    rewritten_descriptions = concatenate_and_rewrite(description, category_description, len(all_possible_data))\n",
    "                    if len(rewritten_descriptions) == column_info[column_name][\"num_categories\"]:\n",
    "                        break\n",
    "                    retry_count += 1\n",
    "                    # print(f\"retry {retry_count} times for {column_name}\")\n",
    "                    if retry_count % 10 == 0:\n",
    "                        print(f\"retry {retry_count} times for {column_name}\")\n",
    "                    if retry_count > 100:\n",
    "                        give_up_columns.append(column_name)\n",
    "                        print(f\"give up column {column_name}\")\n",
    "                        break\n",
    "            if retry_count > 100:\n",
    "                break\n",
    "            if len(all_possible_data) != len(rewritten_descriptions):\n",
    "                map_data_to_description = dict(zip([all_possible_data[0]], rewritten_descriptions))\n",
    "            else:\n",
    "                map_data_to_description = dict(zip(all_possible_data, rewritten_descriptions))\n",
    "            all_concate_result_for_different_rewrite.append(map_data_to_description)\n",
    "        column_info[column_name][\"rewritten_descriptions\"] = all_concate_result_for_different_rewrite\n",
    "\n",
    "save_pkl(column_info, step=3)\n",
    "\n",
    "print(\"give up columns: \", give_up_columns)\n",
    "print(\"giveup number: \", len(give_up_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_120513/1222203965.py:25: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
      "/tmp/ipykernel_120513/1222203965.py:26: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run(column_description=column_description, category_description=category_description, expected_lines=expected_lines)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retry 10 times for updrs_PDNORMAL\n",
      "retry 20 times for updrs_PDNORMAL\n",
      "retry 30 times for updrs_PDNORMAL\n",
      "retry 40 times for updrs_PDNORMAL\n",
      "retry 10 times for updrs_PDNORMAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [01:39<01:39, 99.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retry 10 times for exam_BRADY\n",
      "retry 20 times for exam_BRADY\n",
      "retry 30 times for exam_BRADY\n",
      "retry 40 times for exam_BRADY\n",
      "retry 50 times for exam_BRADY\n",
      "retry 60 times for exam_BRADY\n",
      "retry 70 times for exam_BRADY\n",
      "retry 80 times for exam_BRADY\n",
      "retry 90 times for exam_BRADY\n",
      "retry 100 times for exam_BRADY\n",
      "retry 110 times for exam_BRADY\n",
      "retry 120 times for exam_BRADY\n",
      "retry 130 times for exam_BRADY\n",
      "retry 140 times for exam_BRADY\n",
      "retry 150 times for exam_BRADY\n",
      "retry 160 times for exam_BRADY\n",
      "retry 170 times for exam_BRADY\n",
      "retry 180 times for exam_BRADY\n",
      "retry 190 times for exam_BRADY\n",
      "retry 10 times for exam_BRADY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [07:40<00:00, 230.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column info saved to /openbayes/home/LMTDE/data/datasets/nacc_new/meta/rewrite3/column_info_step3.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "give_up_columns = ['updrs_PDNORMAL', 'exam_BRADY']\n",
    "\n",
    "old_column_info = load_pkl(step=2)\n",
    "column_info = load_pkl(step=3)\n",
    "\n",
    "# try more times\n",
    "for column_name in tqdm(give_up_columns):\n",
    "    assert old_column_info[column_name][\"type\"] == \"categorical\"\n",
    "    all_possible_data = tabular_data[column_name].dropna().unique()\n",
    "    all_possible_data.sort()\n",
    "    all_possible_data = [str(data) for data in all_possible_data]\n",
    "    category_description = \"The column contains the following categories: \" + \", \".join(all_possible_data)\n",
    "    assert(len(all_possible_data) == old_column_info[column_name][\"num_categories\"])\n",
    "\n",
    "    all_concate_result_for_different_rewrite = []\n",
    "    for description in old_column_info[column_name][\"rewritten_descriptions\"]:\n",
    "        if len(all_possible_data) > 10:\n",
    "            rewritten_descriptions = [description]\n",
    "            retry_count = 1001\n",
    "            give_up_columns.append(column_name)\n",
    "        else:\n",
    "            retry_count = 0\n",
    "            while True:\n",
    "                rewritten_descriptions = concatenate_and_rewrite(description, category_description, len(all_possible_data))\n",
    "                if len(rewritten_descriptions) == old_column_info[column_name][\"num_categories\"]:\n",
    "                    break\n",
    "                retry_count += 1\n",
    "                # print(f\"retry {retry_count} times for {column_name}\")\n",
    "                if retry_count % 10 == 0:\n",
    "                    print(f\"retry {retry_count} times for {column_name}\")\n",
    "                if retry_count > 1000:\n",
    "                    give_up_columns.append(column_name)\n",
    "                    print(f\"give up column {column_name}\")\n",
    "                    break\n",
    "        if retry_count > 1000:\n",
    "            break\n",
    "        if len(all_possible_data) != len(rewritten_descriptions):\n",
    "            map_data_to_description = dict(zip([all_possible_data[0]], rewritten_descriptions))\n",
    "        else:\n",
    "            map_data_to_description = dict(zip(all_possible_data, rewritten_descriptions))\n",
    "        all_concate_result_for_different_rewrite.append(map_data_to_description)\n",
    "    old_column_info[column_name][\"rewritten_descriptions\"] = all_concate_result_for_different_rewrite\n",
    "    column_info[column_name] = old_column_info[column_name]\n",
    "\n",
    "\n",
    "save_pkl(column_info, step=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth, we generate the embedding (you can choose any llm or any embedding model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 363/363 [39:56<00:00,  6.60s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column info saved to /openbayes/home/LMTDE/data/datasets/nacc_new/meta/rewrite3/column_info_step4.pkl\n"
     ]
    }
   ],
   "source": [
    "def generate_embeddings(text):\n",
    "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    embedding = embedding_model.embed_query(text)\n",
    "    return embedding\n",
    "\n",
    "column_info = load_pkl(step=3)\n",
    "for column_name in tqdm(column_info):\n",
    "    if len(column_info[column_name][\"rewritten_descriptions\"]) == 0:\n",
    "        continue\n",
    "    if column_info[column_name][\"type\"] == \"categorical\":\n",
    "        assert(isinstance(column_info[column_name][\"rewritten_descriptions\"][0], dict))\n",
    "        embeddings = []\n",
    "        for i in range(len(column_info[column_name][\"rewritten_descriptions\"])):\n",
    "            embeddings_for_key = {}\n",
    "            for key in column_info[column_name][\"rewritten_descriptions\"][i]:\n",
    "                if not isinstance(column_info[column_name][\"rewritten_descriptions\"][i][key], str):\n",
    "                    continue\n",
    "                embedding = generate_embeddings(column_info[column_name][\"rewritten_descriptions\"][i][key])\n",
    "                embeddings_for_key[key] = embedding\n",
    "            embeddings.append(embeddings_for_key)\n",
    "        column_info[column_name][\"embeddings\"] = embeddings\n",
    "    else:\n",
    "        assert(isinstance(column_info[column_name][\"rewritten_descriptions\"][0], str))\n",
    "        embeddings = []\n",
    "        for description in column_info[column_name][\"rewritten_descriptions\"]:\n",
    "            embedding = generate_embeddings(description)\n",
    "            embeddings.append(embedding)\n",
    "        column_info[column_name][\"embeddings\"] = embeddings\n",
    "        \n",
    "save_pkl(column_info, step=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, filter the unexpected columns according to your rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column his_PDOTHR not found in the data\n",
      "column npiq_DEL not found in the data\n",
      "column npiq_HALL not found in the data\n",
      "column npiq_AGIT not found in the data\n",
      "column npiq_DEPD not found in the data\n",
      "column npiq_ANX not found in the data\n",
      "column npiq_ELAT not found in the data\n",
      "column npiq_APA not found in the data\n",
      "column npiq_DISN not found in the data\n",
      "column npiq_IRR not found in the data\n",
      "column npiq_MOT not found in the data\n",
      "column npiq_NITE not found in the data\n",
      "column npiq_APP not found in the data\n",
      "column bat_PENTAGON not found in the data\n",
      "column bat_DIGBACCT not found in the data\n",
      "column bat_DIGBACLS not found in the data\n",
      "column bat_REY1REC not found in the data\n",
      "column bat_REY1INT not found in the data\n",
      "column bat_REY2REC not found in the data\n",
      "column bat_REY2INT not found in the data\n",
      "column bat_REY3REC not found in the data\n",
      "column bat_REY3INT not found in the data\n",
      "column bat_REY4REC not found in the data\n",
      "column bat_REY4INT not found in the data\n",
      "column bat_REY5REC not found in the data\n",
      "column bat_REY5INT not found in the data\n",
      "column bat_REY6REC not found in the data\n",
      "column bat_REY6INT not found in the data\n",
      "column bat_REYDREC not found in the data\n",
      "column bat_REYDINT not found in the data\n",
      "column bat_REYTCOR not found in the data\n",
      "column bat_REYFPOS not found in the data\n"
     ]
    }
   ],
   "source": [
    "column_info = load_pkl(step=4)\n",
    "\n",
    "# example: only want the features in \"column_type.toml\", drop labels and other columns\n",
    "import toml\n",
    "if 'nacc' in tabular_data_path:\n",
    "    toml_path = \"/openbayes/home/LMTDE/data/datasets/nacc_new/meta/conf_mri.toml\"\n",
    "elif 'adni' in tabular_data_path:\n",
    "    toml_path = \"/openbayes/home/LMTDE/data/datasets/adni/meta/adni.toml\"\n",
    "with open(toml_path, \"r\") as f:\n",
    "# with open(\"/openbayes/home/LMTDE/data/datasets/nacc_new/meta/conf_mri.toml\", \"r\") as f:\n",
    "# with open(\"/openbayes/home/LMTDE/data/datasets/adni/meta/adni.toml\", \"r\") as f:\n",
    "    column_type = toml.load(f)\n",
    "    # do anything you want with column_type\n",
    "    # check if column type match\n",
    "    for column_name in column_type['feature']:\n",
    "        if column_name not in column_info:\n",
    "            print(f\"column {column_name} not found in the data\")\n",
    "            continue\n",
    "        if column_info[column_name]['type'] != column_type['feature'][column_name]['type']:\n",
    "            # if category - binary and num_categories=2, match\n",
    "            # if category - multi and num_categories>2, match\n",
    "            if column_info[column_name]['type'] == \"categorical\" and column_type['feature'][column_name]['type'] == \"binary\" and column_info[column_name]['num_categories'] == 2:\n",
    "                continue\n",
    "            if column_info[column_name]['type'] == \"categorical\" and column_type['feature'][column_name]['type'] == \"multiple\" and column_info[column_name]['num_categories'] > 2:\n",
    "                continue\n",
    "            print(f\"column {column_name} type not match, expect {column_type['feature'][column_name]['type']} but got {column_info[column_name]['type']}\")\n",
    "\n",
    "    # drop columns not in column_type\n",
    "    column_info = {column_name: column_info[column_name] for column_name in column_info.keys() if column_name in column_type['feature']}\n",
    "    # save to pkl file\n",
    "    # save_pkl(column_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_info = load_pkl(step=4)\n",
    "# # dump to toml, if want to update the toml\n",
    "# with open(\"/openbayes/home/LMTDE/data/datasets/adni/meta/adni.toml\", \"w\") as f:\n",
    "#     # dump like this\n",
    "#     # [feature.TRAILS]\n",
    "#     # type = \"binary\"\n",
    "#     # num_categories = 2\n",
    "#     for column_name in column_info:\n",
    "#         f.write(f\"\\t[feature.{column_name}]\\n\")\n",
    "#         f.write(f\"\\ttype = \\\"{column_info[column_name]['type']}\\\"\\n\")\n",
    "#         if column_info[column_name]['type'] == \"categorical\":\n",
    "#             f.write(f\"\\tnum_categories = {column_info[column_name]['num_categories']}\\n\")\n",
    "#         else:\n",
    "#             f.write(\"\\tshape = [1]\\n\")\n",
    "#         f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/openbayes/home/LMTDE/data/datasets/nacc_new/meta/rewrite3/column_info.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| column_info[focus_on_this_column]['rewritten_descriptions']: [{'1.0': 'The log day value of 1 indicates that the test was administered on '\n",
      "                                                                          'the first day of the month.  ',\n",
      "                                                                   '12.0': 'The log day value of 12 indicates that the test took place on the '\n",
      "                                                                           'twelfth day of the month.  ',\n",
      "                                                                   '16.0': 'The log day value of 16 signifies that the test was administered on '\n",
      "                                                                           'the sixteenth day of the month.  ',\n",
      "                                                                   '2.0': 'The log day value of 2 signifies that the test was administered on '\n",
      "                                                                          'the second day of the month.  ',\n",
      "                                                                   '25.0': 'The log day value of 25 represents a test conducted on the '\n",
      "                                                                           'twenty-fifth day of the month.  ',\n",
      "                                                                   '27.0': 'The log day value of 27 indicates that the test was administered on '\n",
      "                                                                           'the twenty-seventh day of the month.',\n",
      "                                                                   '6.0': 'The log day value of 6 represents a test that was given on the sixth '\n",
      "                                                                          'day of the month.  '},\n",
      "                                                                  {'1.0': 'This entry captures the date when a specific test was conducted, '\n",
      "                                                                          'which took place on the 1st day of the month.  ',\n",
      "                                                                   '12.0': 'This entry captures the date when a specific test was conducted, '\n",
      "                                                                           'which took place on the 12th day of the month.  ',\n",
      "                                                                   '16.0': 'This entry captures the date when a specific test was conducted, '\n",
      "                                                                           'which took place on the 16th day of the month.  ',\n",
      "                                                                   '2.0': 'This entry captures the date when a specific test was conducted, '\n",
      "                                                                          'which took place on the 2nd day of the month.  ',\n",
      "                                                                   '25.0': 'This entry captures the date when a specific test was conducted, '\n",
      "                                                                           'which took place on the 25th day of the month.  ',\n",
      "                                                                   '27.0': 'This entry captures the date when a specific test was conducted, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363\n",
      "dict_keys(['his_PDOTHRYR', 'cvd_EMOT', 'exam_SLOWINGL', 'updrs_BRADYKIN', 'faq_GAMES', 'bat_MOCAORPL', 'cvd_SOMATIC', 'his_NACCFADM', 'bat_DIGFORSL', 'his_HANDED', 'med_NACCAMD', 'bat_MOCAORDY', 'bat_UDSVERTN', 'med_NACCAHTN', 'updrs_HANDALTR', 'exam_RESTTRL', 'his_HISPOR', 'his_ALCFREQ', 'exam_PARKSIGN', 'cvd_CVDIMAG3', 'updrs_RIGDUPLF', 'bat_MOCACUBE', 'ph_BPDIAS', 'his_NACCFMS', 'his_PDYR', 'bat_CRAFTDVR', 'his_NACCAMS', 'bat_NACCMOCB', 'his_RESIDENC', 'bat_MINTTOTS', 'gds_WRTHLESS', 'his_TOBAC30', 'bat_DIGIBLEN', 'bat_ANIMALS', 'exam_DYSTONR', 'bat_RESPHEAR', 'bat_MOCAORYR', 'his_TIAMULT', 'cvd_CVDIMAG', 'med_ANYMEDS', 'med_NACCAANX', 'bat_UDSVERLC', 'updrs_FACEXP', 'his_STROKMUL', 'bat_TRAILBLI', 'faq_REMDATES', 'bat_MOCAREGI', 'bat_MINTTOTW', 'exam_SOMATL', 'updrs_HANDMOVR', 'exam_SOMATR', 'bat_OTRLARR', 'his_ABUSOTHR', 'his_TBIEXTEN', 'his_HISPANIC', 'his_HYPERCHO', 'exam_ALIENLML', 'bat_RESPOTH', 'his_NACCAGE', 'exam_CORTVISR', 'bat_BOSTON', 'updrs_ARISING', 'his_INDEPEND', 'his_ARTHUPEX', 'his_CVHATT', 'bat_MOCACOMP', 'bat_LOGIDAY', 'med_NACCPDMD', 'ph_HEARWAID', 'his_PTSD', 'cvd_ABRUPT', 'gds_DROPACT', 'ph_HRATE', 'his_CVBYPASS', 'updrs_PDNORMAL', 'bat_UDSBENTC', 'updrs_LEGLF', 'bat_MMSEORLO', 'updrs_GAIT', 'bat_OTRLALI', 'updrs_RIGDUPRT', 'updrs_TAPSLF', 'bat_MOCAREAS', 'updrs_POSSTAB', 'updrs_RIGDLORT', 'updrs_RIGDNECK', 'gds_AFRAID', 'exam_FOCLDEF', 'bat_TRAILA', 'his_NACCFFTD', 'faq_MEALPREP', 'his_TRAUMEXT', 'exam_POSTCORT', 'ph_WEIGHT', 'his_TOBAC100', 'updrs_TRESTRFT', 'his_NCOTHR', 'his_TBIWOLOS', 'bat_OTRLBRR', 'exam_CORTVISL', 'faq_SHOPPING', 'med_NACCNSD', 'bat_DIGFORCT', 'bat_RESPDIST', 'his_RACE', 'his_THYROID', 'bat_MOCACLON', 'cvd_CVDIMAG2', 'his_TRAUMBRF', 'his_CVPACE', 'his_CVPACDEF', 'cvd_FOCLSIGN', 'bat_CRAFTDTI', 'bat_TRAILARR', 'faq_TAXES', 'bat_UDSVERTI', 'updrs_TRACTRHD', 'his_DIABETES', 'gds_STAYHOME', 'his_PSYCDIS', 'exam_RIGIDR', 'his_DEP2YRS', 'bat_MOCADIGI', 'med_NACCEPMD', 'his_DEPOTHR', 'updrs_LEGRT', 'exam_MYOCLLT', 'bat_MINTSCNG', 'bat_OTRAILB', 'his_OCD', 'img_MRI_mIP', 'his_CVOTHR', 'his_NACCAM', 'med_NACCDBMD', 'bat_UDSVERFN', 'his_RBD', 'bat_TRAILB', 'his_CVCHF', 'his_ARTHTYPE', 'his_OTHSLEEP', 'exam_ATAXL', 'bat_MMSECOMP', 'his_HATTMULT', 'exam_CVDMOTR', 'updrs_TRACTLHD', 'med_NACCADMD', 'exam_AXIALPSP', 'his_NACCTBI', 'faq_TRAVEL', 'his_CVANGINA', 'his_BIPOLAR', 'his_SMOKYRS', 'his_PRIMLANG', 'bat_MOCARECN', 'med_NACCEMD', 'ph_VISCORR', 'his_ARTHLOEX', 'bat_UDSVERNF', 'faq_PAYATTN', 'exam_CORTSENL', 'bat_MEMUNITS', 'cvd_CVDCOG', 'exam_PSPCBS', 'bat_COGSTAT', 'img_MRI_FLAIR', 'his_PACKSPER', 'his_MARISTAT', 'his_HYPERTEN', 'gds_NOGDS', 'cvd_STROKCOG', 'img_MRI_T1', 'img_MRI_SWI', 'his_INCONTU', 'bat_MOCATOTS', 'gds_HELPLESS', 'bat_UDSBENTD', 'bat_MOCARECC', 'his_NACCDAD', 'his_NPSYDEV', 'gds_SATIS', 'updrs_SPEECH', 'his_INSOMN', 'his_ALCOCCAS', 'ph_HEIGHT', 'his_NACCREFR', 'bat_LOGIMO', 'gds_WONDRFUL', 'bat_RESPFATG', 'med_NACCACEI', 'exam_ATAXR', 'bat_MMSELOC', 'bat_VNTPCNC', 'exam_OTHNEUR', 'gds_HAPPY', 'bat_NPSYCLOC', 'bat_DIGIB', 'his_CVANGIO', 'his_BIRTHYR', 'bat_RESPEMOT', 'exam_RESTTRR', 'npiq_NPIQINF', 'exam_CVDMOTL', 'his_CBSTROKE', 'exam_POSTINST', 'his_NACCNIHR', 'exam_APRAXSP', 'bat_VNTTOTW', 'cvd_STEPWISE', 'bat_MOCATRAI', 'bat_MMSELAN', 'his_LIVSIT', 'exam_CORTDEF', 'exam_DYSPSP', 'his_HATTYEAR', 'bat_MOCALOC', 'ph_HEARAID', 'exam_GAITDIS', 'faq_STOVE', 'his_NACCFAM', 'his_TRAUMCHR', 'his_NACCSTYR', 'bat_CRAFTCUE', 'bat_MMSEORDA', 'med_NACCAAAS', 'ph_BPSYS', 'bat_MOCBTOTS', 'gds_SPIRITS', 'his_BIRTHMO', 'exam_NORMEXAM', 'bat_MOCAORMO', 'med_NACCVASD', 'faq_BILLS', 'his_NACCREAS', 'bat_LOGIYR', 'ph_VISION', 'bat_MOCACLOH', 'his_SCHIZ', 'bat_RESPINTR', 'bat_NACCMMSE', 'bat_MOCAHEAR', 'his_NACCOMS', 'med_NACCCCBS', 'bat_WAIS', 'bat_CRAFTURS', 'his_NACCOM', 'bat_CRAFTVRS', 'med_NACCAC', 'his_RACETER', 'bat_MOCAORCT', 'cvd_HXHYPER', 'gds_ENERGY', 'bat_UDSVERLN', 'bat_MOCARECR', 'gds_BORED', 'bat_TRAILBRR', 'his_EDUC', 'his_RACESEC', 'bat_DIGIFLEN', 'his_SEX', 'his_APNEA', 'updrs_TRESTRHD', 'his_B12DEF', 'his_NACCMOM', 'bat_NPSYLAN', 'gds_EMPTY', 'exam_MYOCLRT', 'img_MRI_Mag', 'cvd_CVDIMAG1', 'his_ARTHUNK', 'exam_DYSTONL', 'exam_CVDSIGNS', 'ph_HEARING', 'bat_MOCAORDT', 'bat_LOGIPREV', 'bat_RESPDISN', 'exam_NACCNREX', 'bat_MINTPCNC', 'faq_EVENTS', 'bat_MMSEHEAR', 'updrs_TRESTFAC', 'exam_GAITNPH', 'med_NACCAPSY', 'exam_APRAXL', 'bat_DIGIF', 'gds_BETTER', 'exam_CORTSENR', 'exam_SLOWINGR', 'bat_LOGIMEM', 'his_DIABTYPE', 'his_ANXIETY', 'bat_MOCAREPE', 'bat_RESPVAL', 'his_ARTHSPIN', 'bat_MOCAFLUE', 'his_CVHVALVE', 'bat_MOCASER7', 'med_NACCADEP', 'bat_RESPASST', 'med_NACCBETA', 'bat_MOCALETT', 'exam_EYEPSP', 'bat_MOCAVIS', 'exam_APRAXR', 'bat_UDSVERFC', 'bat_MINTSCNC', 'bat_MOCALAN', 'bat_NACCMOCA', 'cvd_CVDIMAG4', 'bat_MOCAABST', 'exam_BRADY', 'bat_OTRLBLI', 'his_TBIYEAR', 'bat_MMSEVIS', 'bat_MODCOMM', 'his_QUITSMOK', 'cvd_HXSTROKE', 'exam_EYEMOVE', 'updrs_RIGDLOLF', 'bat_UDSBENRS', 'exam_GAITPSP', 'updrs_HANDMOVL', 'bat_MEMTIME', 'bat_OTRAILA', 'cvd_FOCLSYM', 'his_TBI', 'updrs_TAPSRT', 'his_TBIBRIEF', 'his_ARTHRIT', 'exam_PARKGAIT', 'cvd_HACHIN', 'gds_NACCGDS', 'exam_RIGIDL', 'med_NACCLIPL', 'his_NACCFM', 'ph_NACCBMI', 'updrs_TRESTLHD', 'gds_MEMPROB', 'exam_SIVDFIND', 'updrs_TRESTLFT', 'exam_ALSFIND', 'ph_VISWCORR', 'bat_TRAILALI', 'bat_MOCACLOC', 'bat_MINTPCNG', 'bat_UDSVERLR', 'gds_HOPELESS', 'his_CBTIA', 'his_SEIZURES', 'updrs_HANDALTL', 'his_INCONTF', 'med_NACCDIUR', 'med_NACCANGI', 'his_ALCOHOL', 'his_PD', 'bat_VEG', 'his_CVAFIB', 'bat_CRAFTDRE', 'bat_UDSVERTE', 'his_NACCTIYR', 'exam_ALIENLMR', 'bat_MOCANAMI', 'updrs_POSTURE', 'med_NACCHTNC'])\n",
      "dict_keys(['type', 'num_categories', 'rewritten_descriptions', 'embeddings'])\n",
      "categorical\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'\n",
      "                                                                           'which took place on the 27th day of the month.',\n",
      "                                                                   '6.0': 'This entry captures the date when a specific test was conducted, '\n",
      "                                                                          'which took place on the 6th day of the month.  '},\n",
      "                                                                  {'1.0': 'This data point indicates that a test was administered on the 1st '\n",
      "                                                                          'day of the month.  ',\n",
      "                                                                   '12.0': 'This data point indicates that a test was administered on the 12th '\n",
      "                                                                           'day of the month.  ',\n",
      "                                                                   '16.0': 'This data point indicates that a test was administered on the 16th '\n",
      "                                                                           'day of the month.  ',\n",
      "                                                                   '2.0': 'This data point indicates that a test was administered on the 2nd '\n",
      "                                                                          'day of the month.  ',\n",
      "                                                                   '25.0': 'This data point indicates that a test was administered on the 25th '\n",
      "                                                                           'day of the month.  ',\n",
      "                                                                   '27.0': 'This data point indicates that a test was administered on the 27th '\n",
      "                                                                           'day of the month.',\n",
      "                                                                   '6.0': 'This data point indicates that a test was administered on the 6th '\n",
      "                                                                          'day of the month.  '}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.  6. 25.  1.  2. 27. 16.]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "focus_on_this_column = \"bat_LOGIDAY\"\n",
    "# have a look at the pkl, do not output embeddings(too large output)\n",
    "print(pkl_path)\n",
    "column_info = load_pkl(step=0)\n",
    "print(len(column_info.keys()))\n",
    "print(column_info.keys())\n",
    "print(column_info[focus_on_this_column].keys())\n",
    "print(column_info[focus_on_this_column]['type'])\n",
    "print(column_info[focus_on_this_column]['num_categories'])\n",
    "ic(column_info[focus_on_this_column]['rewritten_descriptions'])\n",
    "print(tabular_data[focus_on_this_column].dropna().unique())\n",
    "\n",
    "# update_these_columns = []\n",
    "# diff = []\n",
    "# for column_name in column_info:\n",
    "#     if column_info[column_name]['type'] == \"categorical\":\n",
    "#         keys = column_info[column_name]['rewritten_descriptions'][0].keys()\n",
    "#         if 'num_categories' not in column_info[column_name]:\n",
    "#             update_these_columns.append(column_name)\n",
    "#             diff.append(len(keys))\n",
    "#         elif column_info[column_name]['num_categories'] != len(keys):\n",
    "#             # ic(column_name, column_info[column_name]['num_categories'], len(keys))\n",
    "#             update_these_columns.append(column_name)\n",
    "#             # diff.append(len(keys) - column_info[column_name]['num_categories'])\n",
    "# print(update_these_columns)\n",
    "# print(len(update_these_columns))\n",
    "# print(diff)\n",
    "# print(len(diff))\n",
    "# print(sum(diff))\n",
    "# print(len(column_info.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| data_without_nan: 0        0.0\n",
      "                      1        0.0\n",
      "                      2        0.0\n",
      "                      3        0.0\n",
      "                      4        0.0\n",
      "                              ... \n",
      "                      36770    0.0\n",
      "                      36771    0.0\n",
      "                      36772    0.0\n",
      "                      36773    0.0\n",
      "                      36774    0.0\n",
      "                      Name: his_HISPOR, Length: 36697, dtype: float64\n",
      "ic| tabular_data[focus_on_this_column].nunique(): 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_without_nan = tabular_data[focus_on_this_column].dropna()\n",
    "ic(data_without_nan)\n",
    "# print number of unique value\n",
    "ic(tabular_data[focus_on_this_column].nunique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LMTDE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
