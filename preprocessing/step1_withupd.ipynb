{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 data preprocessing\n",
    "\n",
    "part (a): tabular data\n",
    "\n",
    "the zero-shot ability of our model comes from embedding of feature description\n",
    "\n",
    "First we should extract the name of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export http_proxy=http://localhost:1080\n",
    "!export https_proxy=http://localhost:1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "ic.enable()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10601/2555919042.py:5: DtypeWarning: Columns (126) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tabular_data = pd.read_csv(tabular_data_path)\n"
     ]
    }
   ],
   "source": [
    "### NACC dataset\n",
    "\n",
    "tabular_data_path = \"/openbayes/home/LMTDE/data/datasets/nacc_new/naccImg_validation_normed.csv\" # can be any tabular data file, e.g. CSV, TSV, etc.\n",
    "pkl_path = \"/openbayes/home/LMTDE/data/datasets/nacc_new/meta/column_info.pkl\"\n",
    "background = \"Funded by NIA, the National Alzheimer's Coordinating Center (NACC) has developed and maintains a large relational database of standardized clinical and neuropathological research data. NACC provides a valuable resource for both exploratory and explanatory Alzheimer's disease research. NACC data are freely available to all researchers.\"\n",
    "\n",
    "###\n",
    "\n",
    "### ADNI dataset\n",
    "\n",
    "# tabular_data_path = \"/openbayes/home/LMTDE/data/datasets/adni/adni_final.csv\"\n",
    "# pkl_path = \"/openbayes/home/LMTDE/data/datasets/adni/meta/column_info.pkl\"\n",
    "# background = \"The ADNI data set is a comprehensive and widely used collection of longitudinal clinical, imaging, genetic, and other biomarker data. It encompasses various data types, including structural, functional, and molecular brain imaging, biofluid biomarkers, cognitive assessments, genetic data, and demographic information.ADNI participants are assigned to a Schedule of Events (SOE) based on various factors, such as clinical diagnosis (unimpaired, MCI, AD/dementia). During each visit, the SOE dictates what data is collected. This data is made available to approved researchers.\"\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "# read the data into a pandas DataFrame\n",
    "tabular_data = pd.read_csv(tabular_data_path)\n",
    "\n",
    "\n",
    "\n",
    "def save_pkl(column_info, pkl_path=pkl_path, step=0):\n",
    "    # add step to pkl path if step > 0\n",
    "    if step > 0:\n",
    "        pkl_path = pkl_path.replace(\".pkl\", f\"_step{step}.pkl\")\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(column_info, f)\n",
    "    print(f\"column info saved to {pkl_path}\")\n",
    "\n",
    "def load_pkl(pkl_path=pkl_path, step=0):\n",
    "    if step > 0:\n",
    "        pkl_path = pkl_path.replace(\".pkl\", f\"_step{step}.pkl\")\n",
    "    if not os.path.exists(pkl_path):\n",
    "        print(f\"column info not found at {pkl_path}\")\n",
    "        return None\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        column_info = pickle.load(f)\n",
    "    return column_info\n",
    "\n",
    "focus_on_this_column = None\n",
    "columns_that_need_to_update = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, classify the data to {\"categorical\", \"numerical\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column info saved to /openbayes/home/LMTDE/data/datasets/nacc_new/meta/column_info_step1.pkl\n",
      "483\n"
     ]
    }
   ],
   "source": [
    "def what_type_of_data(tabular_data, column_name):\n",
    "    # \"img_emb\": column name contains \"img_\"\n",
    "    # \"numerical\": data type in [int, float] and (unique values > 32 or any float data changes after rounding to 0 decimal places)\n",
    "    # \"categorical\": else\n",
    "    if \"img_\" in column_name:\n",
    "        return \"img_emb\"\n",
    "    if tabular_data[column_name].dtype in [int, float]:\n",
    "        if tabular_data[column_name].nunique() > 10: # 32:\n",
    "            return \"numerical\"\n",
    "        # if any float data changes after rounding to 0 decimal places, then it is numerical\n",
    "        if not tabular_data[column_name].apply(lambda x: round(x, 0)).equals(tabular_data[column_name]):\n",
    "            return \"numerical\"\n",
    "    return \"categorical\"\n",
    "\n",
    "# store the data type of each column in a dictionary\n",
    "column_info = {\n",
    "    column_name: {\n",
    "        \"type\": what_type_of_data(tabular_data, column_name),\n",
    "        \"num_categories\": tabular_data[column_name].nunique()\n",
    "    } for column_name in tabular_data.columns\n",
    "}\n",
    "# column_info[\"MOCA\"].update({\"type\": \"numerical\"})\n",
    "# column_info[\"PTEDUCAT\"].update({\"type\": \"numerical\"})\n",
    "save_pkl(column_info, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we rewrite and concatenate the description and category (for categorical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/483 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483/483 [58:38<00:00,  7.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column info saved to /openbayes/home/LMTDE/data/datasets/nacc_new/meta/column_info_step2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def rewrite_column_description(background, description, rewrite_count=1):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"background\", \"description\", \"rewrite_count\"],\n",
    "        template=\"\"\"\n",
    "        [Background]\n",
    "        Background information: {background}\n",
    "\n",
    "        [Instruction]\n",
    "        Please comprehend and then rewrite the following raw text (which may be a JSON-style string including detailed explanation, or contain only uppercase letters representing abbreviations, in which case you need to infer their meanings based on background knowledge) into a declarative sentence that is easy for humans to understand. Ensure to include the meaning of every possible data value (if provided in the raw text). Rewrite it in {rewrite_count} different ways or styles. (Separate the results with a newline character, no empty lines, and no serial numbers before the results.) Do not include the original column name (the string of uppercase letters) in your rewritten results; express the meaning using normal language instead.\n",
    "\n",
    "        [Raw text]\n",
    "        {description}\n",
    "        \"\"\"\n",
    "    )\n",
    "    # ic(background, description, prompt_template)\n",
    "\n",
    "    llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    result = chain.run(background=background, description=description, rewrite_count=rewrite_count)\n",
    "    rewritten_descriptions = result.strip().split(\"\\n\")\n",
    "    # drop empty line\n",
    "    rewritten_descriptions = [desc for desc in rewritten_descriptions if desc]\n",
    "    assert(len(rewritten_descriptions) >= rewrite_count)\n",
    "    return rewritten_descriptions[:rewrite_count]\n",
    "\n",
    "if 'nacc' in tabular_data_path:\n",
    "    # use additional information to rewrite the column descriptions\n",
    "    addfile_path = \"/openbayes/home/LMTDE/data/datasets/nacc_new/uds3_rdd_origin.xlsx\"\n",
    "    # for each row in addfile, 1th, 2th, ..., 6th column means: VariableName, Form, VariableType, ShortDescriptor, DataType, AllowableCodes\n",
    "    # create a dictionary to store, key=VariableName, value={\"VariableName\":..., \"Form\":..., ...}(json data)\n",
    "    addfile_data = pd.read_excel(addfile_path, sheet_name=\"uds3-rdd\", header=None)\n",
    "    addfile_column_name = addfile_data.iloc[:, 0].values\n",
    "    addfile_info = {}\n",
    "    for i in range(addfile_data.shape[0]):\n",
    "        column_name = addfile_data.iloc[i, 0]\n",
    "        addfile_info[column_name] = {}\n",
    "        addfile_info[column_name][\"origin_column_name\"] = addfile_data.iloc[i, 0]\n",
    "        addfile_info[column_name][\"form\"] = addfile_data.iloc[i, 1]\n",
    "        addfile_info[column_name][\"variable_type\"] = addfile_data.iloc[i, 2]\n",
    "        addfile_info[column_name][\"description\"] = addfile_data.iloc[i, 3]\n",
    "        addfile_info[column_name][\"data_type\"] = addfile_data.iloc[i, 4]\n",
    "        addfile_info[column_name][\"allowable_codes\"] = addfile_data.iloc[i, 5]\n",
    "\n",
    "elif 'adni' in tabular_data_path:\n",
    "    addfile_path = \"/openbayes/home/LMTDE/data/datasets/adni/meta/adni_all_descriptor.csv\"\n",
    "    addfile_data = pd.read_csv(addfile_path)\n",
    "    # get the field name from first row\n",
    "    field_names = addfile_data.columns.tolist()\n",
    "    # ic(field_names)\n",
    "    # for each row\n",
    "    addfile_column_name = addfile_data.iloc[:, 3].values\n",
    "    # ic(addfile_column_name)\n",
    "    addfile_info = {}\n",
    "    for i in range(addfile_data.shape[0]):\n",
    "        column_name = addfile_data.iloc[i, 3]\n",
    "        if column_name not in addfile_info:\n",
    "            addfile_info[column_name] = {}\n",
    "        for j in range(len(field_names)):\n",
    "            # if old info is nan or empty, update it\n",
    "            if field_names[j] not in addfile_info[column_name] or pd.isna(addfile_info[column_name][field_names[j]]) or addfile_info[column_name][field_names[j]] == \"\":\n",
    "                addfile_info[column_name][field_names[j]] = addfile_data.iloc[i, j]\n",
    "\n",
    "else:\n",
    "    assert False, \"tabular_data_path should contain 'nacc' or 'adni'\"\n",
    "\n",
    "    \n",
    "def find_the_most_related_origin_column(column_name, addfile_column_name):\n",
    "    if column_name in addfile_column_name:\n",
    "        # return json string(dump from dict: addfile_info[column_name])\n",
    "        return json.dumps(addfile_info[column_name])\n",
    "    # if column_name is not in addfile_column_name, find the most related column\n",
    "    # use maximum common substring to find the most related column\n",
    "    max_common_substring = \"\"\n",
    "    most_related_column = \"\"\n",
    "    for origin_column_name in addfile_column_name:\n",
    "        if len(origin_column_name) < 5:\n",
    "            continue\n",
    "        for i in range(len(column_name) - 4):\n",
    "            for j in range(i + 5, len(column_name) + 1):\n",
    "                if column_name[i:j] in origin_column_name:\n",
    "                    if j - i > len(max_common_substring):\n",
    "                        max_common_substring = column_name[i:j]\n",
    "                        most_related_column = origin_column_name\n",
    "    # ic(column_name, most_related_column, max_common_substring)\n",
    "    if most_related_column:\n",
    "        return json.dumps(addfile_info[most_related_column])\n",
    "    return column_name\n",
    "\n",
    "\n",
    "column_info = load_pkl(step=1)\n",
    "column_info_next = load_pkl(step=2)\n",
    "\n",
    "\n",
    "for column_name in tqdm(column_info):\n",
    "    if focus_on_this_column != None:\n",
    "        if column_name != focus_on_this_column:\n",
    "            continue\n",
    "    if columns_that_need_to_update:\n",
    "        if column_name not in columns_that_need_to_update:\n",
    "            continue\n",
    "    # description = column_name # you can use other detailed description \n",
    "    description = find_the_most_related_origin_column(column_name, addfile_column_name)\n",
    "\n",
    "    if 'adni' in tabular_data_path:\n",
    "        # based on addfile, use \"CODE\" field to determine the data type\n",
    "        if column_name in addfile_info:\n",
    "            if \"CODE\" in addfile_info[column_name]:\n",
    "                code_str = addfile_info[column_name][\"CODE\"]\n",
    "                # code_str could be nan or empty, skip it\n",
    "                if not pd.isna(code_str) and code_str != \"\":\n",
    "                    if \"..\" in code_str:\n",
    "                        column_info[column_name][\"type\"] = \"numerical\"\n",
    "                    elif \"Step=\" in code_str:\n",
    "                        column_info[column_name][\"type\"] = \"numerical\"\n",
    "    \n",
    "    rewritten_descriptions = rewrite_column_description(background, description)\n",
    "    column_info[column_name][\"rewritten_descriptions\"] = rewritten_descriptions\n",
    "    # ic(rewritten_descriptions)\n",
    "    # ic(column_info_next[column_name])\n",
    "    if columns_that_need_to_update:\n",
    "        column_info_next[column_name] = column_info[column_name]\n",
    "\n",
    "\n",
    "if columns_that_need_to_update:\n",
    "    save_pkl(column_info_next, step=2)\n",
    "elif focus_on_this_column is None:\n",
    "    save_pkl(column_info, step=2)\n",
    "save_pkl(column_info, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/483 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retry 1 times for exam_CORTDEF\n",
      "retry 2 times for exam_CORTDEF\n",
      "retry 3 times for exam_CORTDEF\n",
      "retry 4 times for exam_CORTDEF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483/483 [00:33<00:00, 14.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column info saved to /openbayes/home/LMTDE/data/datasets/nacc_new/meta/column_info_step3.pkl\n",
      "give up columns:  []\n",
      "giveup number:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def concatenate_and_rewrite(column_description, category_description, expected_lines=None):\n",
    "    if len(category_description) > 2000:\n",
    "        ic(\"category_description is too long, rejected\")\n",
    "        ic(column_description)\n",
    "        return [column_description]\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"column_description\", \"category_description\", \"expected_lines\"],\n",
    "        template=\"\"\"\n",
    "        [Instruction]\n",
    "        The following text describes one of the columns in the tabular data (categorical data) and includes the category descriptions that actually exist in the real data (some data mentioned in the column description may not exist in the real data). Please concatenate the column description with each of the category descriptions and rewrite it as a declarative sentence (utilizing the description of the category it belongs to). \n",
    "        1. Because the real data contains {expected_lines} categories, you should output exactly {expected_lines} lines. Do not output these results in only one line. (Separate the results with a line break, no empty lines, and no serial numbers before the results.) \n",
    "        2. Do not include the original column name (the string of uppercase letters) in your rewritten results; express the meaning using normal language instead.\n",
    "\n",
    "        [Column description]\n",
    "        {column_description}\n",
    "\n",
    "        [Category description]\n",
    "        {category_description}\n",
    "        \"\"\"\n",
    "    )\n",
    "    #  Limit each response to 30 words or fewer. If there are particularly many categories (more than 10), limit each response to 15 words or fewer.\n",
    "    #  Do not include the original column name (the string of uppercase letters) in your rewritten results; express the meaning using normal language instead.\n",
    "    #  Later this rewrite result will be used to concatenate with each category data, so you should preserve enough important information.\n",
    "    llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    result = chain.run(column_description=column_description, category_description=category_description, expected_lines=expected_lines)\n",
    "    rewritten_descriptions = result.strip().split(\"\\n\")\n",
    "    # drop empty line\n",
    "    rewritten_descriptions = [desc for desc in rewritten_descriptions if desc]\n",
    "    return rewritten_descriptions\n",
    "\n",
    "\n",
    "column_info = load_pkl(step=2)\n",
    "column_info_next = load_pkl(step=3)\n",
    "columns_that_need_to_update = ['exam_CORTDEF']\n",
    "give_up_columns = []\n",
    "for column_name in tqdm(column_info):\n",
    "    if focus_on_this_column != None:\n",
    "        if column_name != focus_on_this_column:\n",
    "            continue\n",
    "    if columns_that_need_to_update:\n",
    "        if column_name not in columns_that_need_to_update:\n",
    "            continue\n",
    "    if column_info[column_name][\"type\"] == \"categorical\":\n",
    "        all_possible_data = tabular_data[column_name].dropna().unique()\n",
    "        all_possible_data.sort()\n",
    "        all_possible_data = [str(data) for data in all_possible_data]\n",
    "        category_description = \"The column contains the following categories: \" + \", \".join(all_possible_data)\n",
    "        assert(len(all_possible_data) == column_info[column_name][\"num_categories\"])\n",
    "        retry_count = 0\n",
    "        while True:\n",
    "            all_concate_result_for_different_rewrite = []\n",
    "            for description in column_info[column_name][\"rewritten_descriptions\"]:\n",
    "                if len(all_possible_data) > 40:\n",
    "                    rewritten_descriptions = [description]\n",
    "                else:\n",
    "                    rewritten_descriptions = concatenate_and_rewrite(description, category_description, len(all_possible_data))\n",
    "                if len(all_possible_data) != len(rewritten_descriptions):\n",
    "                    map_data_to_description = dict(zip([all_possible_data[0]], rewritten_descriptions))\n",
    "                else:\n",
    "                    map_data_to_description = dict(zip(all_possible_data, rewritten_descriptions))\n",
    "                all_concate_result_for_different_rewrite.append(map_data_to_description)\n",
    "            column_info[column_name][\"rewritten_descriptions\"] = all_concate_result_for_different_rewrite\n",
    "            if len(rewritten_descriptions) == column_info[column_name][\"num_categories\"]:\n",
    "                break\n",
    "            retry_count += 1\n",
    "            print(f\"retry {retry_count} times for {column_name}\")\n",
    "            if retry_count > 10:\n",
    "                give_up_columns.append(column_name)\n",
    "                break\n",
    "    if columns_that_need_to_update:\n",
    "        column_info_next[column_name] = column_info[column_name]\n",
    "        # ic(column_name, column_info_next[column_name])\n",
    "\n",
    "if columns_that_need_to_update:\n",
    "    save_pkl(column_info_next, step=3)\n",
    "elif focus_on_this_column is None:\n",
    "    save_pkl(column_info, step=3)\n",
    "\n",
    "print(\"give up columns: \", give_up_columns)\n",
    "print(\"giveup number: \", len(give_up_columns))\n",
    "\n",
    "column_info_updated = load_pkl(step=3)\n",
    "for column_name in columns_that_need_to_update:\n",
    "    # ic(column_name, column_info_updated[column_name][\"rewritten_descriptions\"][0].keys())\n",
    "    pass\n",
    "\n",
    "# columns_that_need_to_update = ['his_ARTHTYPX', 'updrs_HANDMVRX', 'NACCID', 'updrs_SPEECHX', 'his_PSYCDISX', 'his_CVOTHRX', 'updrs_TRESTLFX', 'his_ABUSX', 'cvd_CVDIMAGX', 'his_RACESECX', 'updrs_TAPSLFX', 'his_NACCOMX', 'updrs_HANDMVLX', 'updrs_LEGRTX', 'updrs_POSTUREX', 'updrs_HANDATRX', 'ODE', 'his_PRIMLANX', 'his_NCOTHRX', 'updrs_RIGDLORX', 'updrs_HANDATLX', 'updrs_RIGDUPRX', 'his_RACEX', 'bat_NPSYLANX', 'updrs_BRADYKIX', 'exam_CORTDEF', 'updrs_TRACTLHX', 'updrs_POSSTABX', 'updrs_ARISINGX', 'exam_OTHNEURX', 'gds_ENERGY', 'updrs_GAITX', 'updrs_TAPSRTX', 'his_OTHSLEEX', 'exam_APRAXL', 'updrs_RIGDUPLX', 'updrs_RIGDNEX', 'npiq_NPIQINFX', 'his_HISPORX', 'updrs_LEGLFX', 'updrs_TRACTRHX', 'updrs_RIGDLOLX', 'VISITDATE']\n",
    "# still_need_to_update = []\n",
    "# for column_name in columns_that_need_to_update:\n",
    "#     # if num_categories matches the number of rewritten descriptions, then it is already updated\n",
    "#     if column_info_updated[column_name][\"num_categories\"] == len(column_info_updated[column_name][\"rewritten_descriptions\"][0].keys()):\n",
    "#         continue\n",
    "#     still_need_to_update.append(column_name)\n",
    "# columns_that_need_to_update = still_need_to_update\n",
    "# print(len(columns_that_need_to_update))\n",
    "# print(columns_that_need_to_update)\n",
    "# ic(column_info_updated[\"GDWORTH\"])\n",
    "# ic(tabular_data[\"GDWORTH\"].dropna().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth, we generate the embedding (you can choose any llm or any embedding model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/483 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483/483 [2:08:52<00:00, 16.01s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column info saved to /openbayes/home/LMTDE/data/datasets/nacc_new/meta/column_info_step4.pkl\n"
     ]
    }
   ],
   "source": [
    "def generate_embeddings(text):\n",
    "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    embedding = embedding_model.embed_query(text)\n",
    "    return embedding\n",
    "\n",
    "focus_on_this_column = None\n",
    "columns_that_need_to_update = []\n",
    "column_info = load_pkl(step=3)\n",
    "column_info_next = load_pkl(step=4)\n",
    "for column_name in tqdm(column_info):\n",
    "    if focus_on_this_column != None:\n",
    "        if column_name != focus_on_this_column:\n",
    "            continue\n",
    "    if columns_that_need_to_update:\n",
    "        if column_name not in columns_that_need_to_update:\n",
    "            continue\n",
    "    if column_info[column_name][\"type\"] == \"categorical\":\n",
    "        assert(isinstance(column_info[column_name][\"rewritten_descriptions\"][0], dict))\n",
    "        embeddings = []\n",
    "        for i in range(len(column_info[column_name][\"rewritten_descriptions\"])):\n",
    "            embeddings_for_key = {}\n",
    "            for key in column_info[column_name][\"rewritten_descriptions\"][i]:\n",
    "                if not isinstance(column_info[column_name][\"rewritten_descriptions\"][i][key], str):\n",
    "                    continue\n",
    "                embedding = generate_embeddings(column_info[column_name][\"rewritten_descriptions\"][i][key])\n",
    "                embeddings_for_key[key] = embedding\n",
    "            embeddings.append(embeddings_for_key)\n",
    "        column_info[column_name][\"embeddings\"] = embeddings\n",
    "    else:\n",
    "        assert(isinstance(column_info[column_name][\"rewritten_descriptions\"][0], str))\n",
    "        embeddings = []\n",
    "        for description in column_info[column_name][\"rewritten_descriptions\"]:\n",
    "            embedding = generate_embeddings(description)\n",
    "            embeddings.append(embedding)\n",
    "        column_info[column_name][\"embeddings\"] = embeddings\n",
    "    if columns_that_need_to_update:\n",
    "        column_info_next[column_name] = column_info[column_name]\n",
    "        \n",
    "if columns_that_need_to_update:\n",
    "    save_pkl(column_info_next, step=4)\n",
    "elif focus_on_this_column is None:\n",
    "    save_pkl(column_info, step=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, filter the unexpected columns according to your rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column his_PDOTHR not found in the data\n",
      "column npiq_DEL not found in the data\n",
      "column npiq_HALL not found in the data\n",
      "column npiq_AGIT not found in the data\n",
      "column npiq_DEPD not found in the data\n",
      "column npiq_ANX not found in the data\n",
      "column npiq_ELAT not found in the data\n",
      "column npiq_APA not found in the data\n",
      "column npiq_DISN not found in the data\n",
      "column npiq_IRR not found in the data\n",
      "column npiq_MOT not found in the data\n",
      "column npiq_NITE not found in the data\n",
      "column npiq_APP not found in the data\n",
      "column his_BIRTHMO type not match, expect categorical but got numerical\n",
      "column bat_PENTAGON not found in the data\n",
      "column bat_LOGIMO type not match, expect categorical but got numerical\n",
      "column bat_LOGIDAY type not match, expect categorical but got numerical\n",
      "column bat_DIGBACCT not found in the data\n",
      "column bat_DIGBACLS not found in the data\n",
      "column bat_REY1REC not found in the data\n",
      "column bat_REY1INT not found in the data\n",
      "column bat_REY2REC not found in the data\n",
      "column bat_REY2INT not found in the data\n",
      "column bat_REY3REC not found in the data\n",
      "column bat_REY3INT not found in the data\n",
      "column bat_REY4REC not found in the data\n",
      "column bat_REY4INT not found in the data\n",
      "column bat_REY5REC not found in the data\n",
      "column bat_REY5INT not found in the data\n",
      "column bat_REY6REC not found in the data\n",
      "column bat_REY6INT not found in the data\n",
      "column bat_REYDREC not found in the data\n",
      "column bat_REYDINT not found in the data\n",
      "column bat_REYTCOR not found in the data\n",
      "column bat_REYFPOS not found in the data\n",
      "column info saved to /openbayes/home/LMTDE/data/datasets/nacc_new/meta/column_info.pkl\n"
     ]
    }
   ],
   "source": [
    "column_info = load_pkl(step=4)\n",
    "\n",
    "# example: only want the features in \"column_type.toml\", drop labels and other columns\n",
    "import toml\n",
    "with open(\"/openbayes/home/LMTDE/data/datasets/nacc_new/meta/conf_mri.toml\", \"r\") as f:\n",
    "# with open(\"/openbayes/home/LMTDE/data/datasets/adni/meta/adni.toml\", \"r\") as f:\n",
    "    column_type = toml.load(f)\n",
    "    # do anything you want with column_type\n",
    "    # check if column type match\n",
    "    for column_name in column_type['feature']:\n",
    "        if column_name not in column_info:\n",
    "            print(f\"column {column_name} not found in the data\")\n",
    "            continue\n",
    "        if column_info[column_name]['type'] != column_type['feature'][column_name]['type']:\n",
    "            # if category - binary and num_categories=2, match\n",
    "            # if category - multi and num_categories>2, match\n",
    "            if column_info[column_name]['type'] == \"categorical\" and column_type['feature'][column_name]['type'] == \"binary\" and column_info[column_name]['num_categories'] == 2:\n",
    "                continue\n",
    "            if column_info[column_name]['type'] == \"categorical\" and column_type['feature'][column_name]['type'] == \"multiple\" and column_info[column_name]['num_categories'] > 2:\n",
    "                continue\n",
    "            print(f\"column {column_name} type not match, expect {column_type['feature'][column_name]['type']} but got {column_info[column_name]['type']}\")\n",
    "\n",
    "    # drop columns not in column_type\n",
    "    column_info = {column_name: column_info[column_name] for column_name in column_info.keys() if column_name in column_type['feature']}\n",
    "    # save to pkl file\n",
    "    save_pkl(column_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_info = load_pkl(step=4)\n",
    "# # dump to toml, if want to update the toml\n",
    "# with open(\"/openbayes/home/LMTDE/data/datasets/adni/meta/adni.toml\", \"w\") as f:\n",
    "#     # dump like this\n",
    "#     # [feature.TRAILS]\n",
    "#     # type = \"binary\"\n",
    "#     # num_categories = 2\n",
    "#     for column_name in column_info:\n",
    "#         f.write(f\"\\t[feature.{column_name}]\\n\")\n",
    "#         f.write(f\"\\ttype = \\\"{column_info[column_name]['type']}\\\"\\n\")\n",
    "#         if column_info[column_name]['type'] == \"categorical\":\n",
    "#             f.write(f\"\\tnum_categories = {column_info[column_name]['num_categories']}\\n\")\n",
    "#         else:\n",
    "#             f.write(\"\\tshape = [1]\\n\")\n",
    "#         f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| column_info[focus_on_this_column]['rewritten_descriptions']: [{'0.0': 'The data indicates that the respondent identifies as White.  ',\n",
      "                                                                   '1.0': 'The data indicates that the respondent identifies as Black or '\n",
      "                                                                          'African American.  ',\n",
      "                                                                   '2.0': 'The data indicates that the respondent identifies as American Indian '\n",
      "                                                                          'or Alaska Native.  ',\n",
      "                                                                   '3.0': 'The data indicates that the respondent identifies as Native Hawaiian '\n",
      "                                                                          'or Other Pacific Islander.  ',\n",
      "                                                                   '4.0': 'The data indicates that the respondent identifies as Asian.  ',\n",
      "                                                                   '5.0': 'The data indicates that no race was reported.'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483\n",
      "dict_keys(['npiq_APPSEV', 'bat_RAY5REC', 'his_ARTHTYPX', 'his_PDOTHRYR', 'cvd_EMOT', 'exam_SLOWINGL', 'updrs_HANDMVRX', 'updrs_BRADYKIN', 'faq_GAMES', 'bat_RAY6INT', 'bat_MOCAORPL', 'bat_RAY2REC', 'cvd_SOMATIC', 'his_NACCFADM', 'bat_DIGFORSL', 'his_HANDED', 'med_NACCAMD', 'bat_MOCAORDY', 'bat_UDSVERTN', 'med_NACCAHTN', 'updrs_HANDALTR', 'exam_RESTTRL', 'his_HISPOR', 'his_ALCFREQ', 'exam_PARKSIGN', 'cvd_CVDIMAG3', 'bat_RAYDINT', 'updrs_RIGDUPLF', 'bat_MOCACUBE', 'ph_BPDIAS', 'NACCID', 'updrs_SPEECHX', 'ALCDEM', 'his_NACCFMS', 'his_PDYR', 'bat_MENTAGON', 'bat_CRAFTDVR', 'his_NACCAMS', 'ANXIET', 'bat_NACCMOCB', 'his_RESIDENC', 'bat_MINTTOTS', 'gds_WRTHLESS', 'his_TOBAC30', 'bat_DIGIBLEN', 'bat_ANIMALS', 'exam_DYSTONR', 'bat_RESPHEAR', 'bat_MOCAORYR', 'his_TIAMULT', 'cvd_CVDIMAG', 'med_ANYMEDS', 'med_NACCAANX', 'bat_UDSVERLC', 'updrs_FACEXP', 'his_STROKMUL', 'npiq_NITESEV', 'bat_TRAILBLI', 'faq_REMDATES', 'npiq_HALLSEV', 'his_PSYCDISX', 'bat_MOCAREGI', 'cdr_CDRSUM', 'bat_MINTTOTW', 'exam_SOMATL', 'updrs_HANDMOVR', 'exam_SOMATR', 'bat_OTRLARR', 'his_ABUSOTHR', 'bat_RAY3INT', 'his_TBIEXTEN', 'his_HISPANIC', 'his_HYPERCHO', 'exam_ALIENLML', 'bat_RESPOTH', 'DEP', 'his_NACCAGE', 'exam_CORTVISR', 'bat_BOSTON', 'updrs_ARISING', 'his_INDEPEND', 'his_ARTHUPEX', 'his_CVHATT', 'bat_MOCACOMP', 'bat_LOGIDAY', 'med_NACCPDMD', 'ph_HEARWAID', 'his_PTSD', 'cvd_ABRUPT', 'gds_DROPACT', 'ph_HRATE', 'his_CVBYPASS', 'updrs_PDNORMAL', 'bat_UDSBENTC', 'his_CVOTHRX', 'updrs_LEGLF', 'bat_MMSEORLO', 'updrs_GAIT', 'DYSILL', 'bat_OTRLALI', 'DE', 'updrs_RIGDUPRT', 'updrs_TRESTLFX', 'updrs_TAPSLF', 'his_ABUSX', 'bat_MOCAREAS', 'updrs_POSSTAB', 'updrs_RIGDLORT', 'updrs_RIGDNECK', 'gds_AFRAID', 'exam_FOCLDEF', 'bat_TRAILA', 'npiq_DEPDSEV', 'updrs_TRESTFAX', 'his_NACCFFTD', 'faq_MEALPREP', 'his_TRAUMEXT', 'exam_POSTCORT', 'ph_WEIGHT', 'cvd_CVDIMAGX', 'bat_RESPOTHX', 'his_TOBAC100', 'SEF', 'updrs_TRESTRFT', 'his_RACESECX', 'his_NCOTHR', 'his_RACETERX', 'LBD', 'npiq_DELSEV', 'his_TBIWOLOS', 'bat_OTRLBRR', 'exam_CORTVISL', 'faq_SHOPPING', 'med_NACCNSD', 'bat_DIGFORCT', 'bat_RESPDIST', 'his_RACE', 'his_THYROID', 'bat_MOCACLON', 'updrs_TAPSLFX', 'cvd_CVDIMAG2', 'his_TRAUMBRF', 'his_CVPACE', 'his_CVPACDEF', 'cvd_FOCLSIGN', 'bat_CRAFTDTI', 'bat_TRAILARR', 'his_NACCOMX', 'faq_TAXES', 'npiq_MOTSEV', 'bat_UDSVERTI', 'updrs_TRACTRHD', 'his_DIABETES', 'gds_STAYHOME', 'his_PSYCDIS', 'exam_RIGIDR', 'VD', 'updrs_HANDMVLX', 'IMPSUB', 'his_DEP2YRS', 'bat_MOCADIGI', 'med_NACCEPMD', 'FTD', 'his_DEPOTHR', 'updrs_LEGRT', 'bat_RAY1INT', 'exam_MYOCLLT', 'bat_MINTSCNG', 'PARK', 'bat_OTRAILB', 'his_OCD', 'TBI', 'img_MRI_mIP', 'updrs_LEGRTX', 'updrs_POSTUREX', 'his_CVOTHR', 'his_NACCAM', 'med_NACCDBMD', 'bat_UDSVERFN', 'his_RBD', 'bat_TRAILB', 'MCI', 'his_CVCHF', 'bat_MMSELANX', 'bat_RAY2INT', 'his_ARTHTYPE', 'updrs_HANDATRX', 'his_OTHSLEEP', 'OTHCOG', 'cdr_CDRGLOB', 'ODE', 'exam_ATAXL', 'bat_MMSECOMP', 'his_HATTMULT', 'exam_CVDMOTR', 'updrs_TRACTLHD', 'med_NACCADMD', 'exam_AXIALPSP', 'his_NACCTBI', 'bat_RAY5INT', 'faq_TRAVEL', 'his_CVANGINA', 'his_BIPOLAR', 'his_PRIMLANX', 'his_SMOKYRS', 'his_PRIMLANG', 'bat_MOCARECN', 'med_NACCEMD', 'ph_VISCORR', 'his_ARTHLOEX', 'updrs_TRESTRFX', 'bat_RAYFPOS', 'bat_UDSVERNF', 'faq_PAYATTN', 'exam_CORTSENL', 'bat_MEMUNITS', 'his_NCOTHRX', 'cvd_CVDCOG', 'PSP', 'bat_RAY6REC', 'exam_PSPCBS', 'bat_COGSTAT', 'img_MRI_FLAIR', 'his_PACKSPER', 'his_MARISTAT', 'his_HYPERTEN', 'gds_NOGDS', 'cvd_STROKCOG', 'npiq_AGITSEV', 'img_MRI_T1', 'img_MRI_SWI', 'bat_RAY3REC', 'his_INCONTU', 'bat_MOCATOTS', 'updrs_RIGDLORX', 'gds_HELPLESS', 'bat_UDSBENTD', 'updrs_HANDATLX', 'updrs_RIGDUPRX', 'bat_MOCARECC', 'his_NACCDAD', 'his_NPSYDEV', 'gds_SATIS', 'updrs_SPEECH', 'his_NACCAMSX', 'his_INSOMN', 'his_ALCOCCAS', 'ph_HEIGHT', 'his_NACCREFR', 'bat_LOGIMO', 'gds_WONDRFUL', 'bat_RESPFATG', 'bat_RAYTCOR', 'his_NACCOMSX', 'med_NACCACEI', 'exam_ATAXR', 'bat_MMSELOC', 'bat_VNTPCNC', 'his_RACEX', 'exam_OTHNEUR', 'gds_HAPPY', 'bat_NPSYCLOC', 'bat_DIGIB', 'his_CVANGIO', 'bat_NPSYLANX', 'updrs_BRADYKIX', 'his_BIRTHYR', 'bat_RESPEMOT', 'exam_RESTTRR', 'npiq_NPIQINF', 'exam_CVDMOTL', 'his_CBSTROKE', 'exam_POSTINST', 'his_NACCNIHR', 'exam_APRAXSP', 'bat_VNTTOTW', 'cvd_STEPWISE', 'bat_MOCATRAI', 'bat_RAY1REC', 'bat_MOCALANX', 'bat_MMSELAN', 'his_LIVSIT', 'exam_CORTDEF', 'exam_DYSPSP', 'his_HATTYEAR', 'bat_MOCALOC', 'ph_HEARAID', 'exam_GAITDIS', 'faq_STOVE', 'apoe_NACCNE4S', 'his_NACCFAM', 'updrs_TRESTRHX', 'his_TRAUMCHR', 'his_NACCSTYR', 'updrs_TRACTLHX', 'bat_CRAFTCUE', 'bat_MMSEORDA', 'med_NACCAAAS', 'his_NACCFMSX', 'ph_BPSYS', 'bat_MOCBTOTS', 'PSY', 'gds_SPIRITS', 'his_BIRTHMO', 'npiq_DISNSEV', 'exam_NORMEXAM', 'updrs_TRESTLHX', 'bat_MOCAORMO', 'med_NACCVASD', 'faq_BILLS', 'his_NACCREAS', 'bat_LOGIYR', 'bat_RAYDREC', 'updrs_POSSTABX', 'ph_VISION', 'updrs_ARISINGX', 'bat_MOCACLOH', 'his_SCHIZ', 'bat_RESPINTR', 'bat_NACCMMSE', 'bat_MOCAHEAR', 'his_NACCOMS', 'med_NACCCCBS', 'bat_WAIS', 'bat_CRAFTURS', 'his_NACCOM', 'STROKE', 'PDOTHR', 'npiq_IRRSEV', 'bat_CRAFTVRS', 'BIPOLDX', 'med_NACCAC', 'his_RACETER', 'bat_MOCAORCT', 'cvd_HXHYPER', 'exam_OTHNEURX', 'gds_ENERGY', 'bat_UDSVERLN', 'bat_MOCARECR', 'gds_BORED', 'bat_TRAILBRR', 'his_EDUC', 'his_RACESEC', 'bat_DIGIFLEN', 'his_SEX', 'his_APNEA', 'HUNT', 'updrs_TRESTRHD', 'his_B12DEF', 'his_NACCMOM', 'bat_NPSYLAN', 'gds_EMPTY', 'updrs_GAITX', 'NC', 'exam_MYOCLRT', 'img_MRI_Mag', 'cvd_CVDIMAG1', 'his_ARTHUNK', 'his_NACCFMX', 'updrs_TAPSRTX', 'exam_DYSTONL', 'exam_CVDSIGNS', 'npiq_ANXSEV', 'ph_HEARING', 'bat_MOCAORDT', 'his_OTHSLEEX', 'AD', 'bat_LOGIPREV', 'bat_RESPDISN', 'exam_NACCNREX', 'his_NACCAGEB', 'bat_MINTPCNC', 'faq_EVENTS', 'bat_MMSEHEAR', 'PPAPH', 'updrs_TRESTFAC', 'PRD', 'exam_GAITNPH', 'med_NACCAPSY', 'exam_APRAXL', 'bat_DIGIF', 'gds_BETTER', 'exam_CORTSENR', 'exam_SLOWINGR', 'bat_LOGIMEM', 'updrs_RIGDUPLX', 'DELIR', 'PTSDDX', 'his_DIABTYPE', 'his_ANXIETY', 'bat_MOCAREPE', 'bat_RESPVAL', 'his_ARTHSPIN', 'bat_MOCAFLUE', 'his_CVHVALVE', 'bat_MOCASER7', 'med_NACCADEP', 'bat_RESPASST', 'MSA', 'med_NACCBETA', 'updrs_RIGDNEX', 'bat_MOCALETT', 'NPH', 'exam_EYEPSP', 'his_NACCAMX', 'bat_MOCAVIS', 'NACCLBDS', 'exam_APRAXR', 'bat_UDSVERFC', 'bat_MINTSCNC', 'bat_MOCALAN', 'bat_NACCMOCA', 'cvd_CVDIMAG4', 'CBD', 'bat_MOCAABST', 'exam_BRADY', 'bat_OTRLBLI', 'bat_RAY4REC', 'npiq_ELATSEV', 'his_TBIYEAR', 'bat_MMSEVIS', 'npiq_APASEV', 'bat_MODCOMM', 'bat_RAY4INT', 'his_QUITSMOK', 'cvd_HXSTROKE', 'exam_EYEMOVE', 'updrs_RIGDLOLF', 'bat_UDSBENRS', 'exam_GAITPSP', 'updrs_HANDMOVL', 'bat_MEMTIME', 'bat_OTRAILA', 'cvd_FOCLSYM', 'his_TBI', 'updrs_TAPSRT', 'his_TBIBRIEF', 'NEOP', 'npiq_NPIQINFX', 'his_ARTHRIT', 'exam_PARKGAIT', 'cvd_HACHIN', 'gds_NACCGDS', 'exam_RIGIDL', 'updrs_FACEXPX', 'his_HISPORX', 'med_NACCLIPL', 'his_NACCFM', 'ph_NACCBMI', 'updrs_LEGLFX', 'updrs_TRESTLHD', 'gds_MEMPROB', 'exam_SIVDFIND', 'updrs_TRESTLFT', 'exam_ALSFIND', 'ph_VISWCORR', 'bat_TRAILALI', 'bat_MOCACLOC', 'updrs_TRACTRHX', 'updrs_RIGDLOLX', 'bat_MINTPCNG', 'bat_UDSVERLR', 'gds_HOPELESS', 'his_CBTIA', 'his_SEIZURES', 'updrs_HANDALTL', 'his_INCONTF', 'med_NACCDIUR', 'med_NACCANGI', 'his_ALCOHOL', 'his_PD', 'bat_VEG', 'his_CVAFIB', 'bat_CRAFTDRE', 'bat_UDSVERTE', 'HIV', 'his_NACCTIYR', 'exam_ALIENLMR', 'bat_MOCANAMI', 'updrs_POSTURE', 'med_NACCHTNC', 'SCHIZOP', 'VISITDATE'])\n",
      "dict_keys(['type', 'num_categories', 'rewritten_descriptions'])\n",
      "categorical\n",
      "6\n",
      "[3. 0. 5. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "focus_on_this_column = \"his_RACETER\"\n",
    "# have a look at the pkl, do not output embeddings(too large output)\n",
    "column_info = load_pkl(step=3)\n",
    "print(len(column_info.keys()))\n",
    "print(column_info.keys())\n",
    "print(column_info[focus_on_this_column].keys())\n",
    "print(column_info[focus_on_this_column]['type'])\n",
    "print(column_info[focus_on_this_column]['num_categories'])\n",
    "ic(column_info[focus_on_this_column]['rewritten_descriptions'])\n",
    "print(tabular_data[focus_on_this_column].dropna().unique())\n",
    "\n",
    "# update_these_columns = []\n",
    "# diff = []\n",
    "# for column_name in column_info:\n",
    "#     if column_info[column_name]['type'] == \"categorical\":\n",
    "#         keys = column_info[column_name]['rewritten_descriptions'][0].keys()\n",
    "#         if 'num_categories' not in column_info[column_name]:\n",
    "#             update_these_columns.append(column_name)\n",
    "#             diff.append(len(keys))\n",
    "#         elif column_info[column_name]['num_categories'] != len(keys):\n",
    "#             # ic(column_name, column_info[column_name]['num_categories'], len(keys))\n",
    "#             update_these_columns.append(column_name)\n",
    "#             # diff.append(len(keys) - column_info[column_name]['num_categories'])\n",
    "# print(update_these_columns)\n",
    "# print(len(update_these_columns))\n",
    "# print(diff)\n",
    "# print(len(diff))\n",
    "# print(sum(diff))\n",
    "# print(len(column_info.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| data_without_nan: 0        0.0\n",
      "                      1        0.0\n",
      "                      2        0.0\n",
      "                      3        0.0\n",
      "                      4        0.0\n",
      "                              ... \n",
      "                      36770    0.0\n",
      "                      36771    0.0\n",
      "                      36772    0.0\n",
      "                      36773    0.0\n",
      "                      36774    0.0\n",
      "                      Name: his_HISPOR, Length: 36697, dtype: float64\n",
      "ic| tabular_data[focus_on_this_column].nunique(): 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_without_nan = tabular_data[focus_on_this_column].dropna()\n",
    "ic(data_without_nan)\n",
    "# print number of unique value\n",
    "ic(tabular_data[focus_on_this_column].nunique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LMTDE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
